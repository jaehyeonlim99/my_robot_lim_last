#!/usr/bin/env python3
import torch
from ultralytics import YOLO
from transformers import BlipProcessor, BlipForConditionalGeneration, BlipForQuestionAnswering
from PIL import Image as PILImage
import cv2
import threading
import sys
import time


class BLIP_YOLO_Webcam:
    def __init__(self):
        print("ğŸš€ BLIP + YOLO Node Starting...")

        # âœ… Device ì„¤ì •
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ“± Using device: {self.device}")

        # âœ… BLIP ëª¨ë¸ (Caption + VQA)
        self.caption_model_name = "Salesforce/blip-image-captioning-large"
        self.vqa_model_name = "Salesforce/blip-vqa-base"

        self.caption_processor = BlipProcessor.from_pretrained(self.caption_model_name)
        self.caption_model = BlipForConditionalGeneration.from_pretrained(
            self.caption_model_name,
            torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32
        ).to(self.device)

        self.vqa_processor = BlipProcessor.from_pretrained(self.vqa_model_name)
        self.vqa_model = BlipForQuestionAnswering.from_pretrained(
            self.vqa_model_name,
            torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32
        ).to(self.device)

        # âœ… YOLOv8 ëª¨ë¸
        print("ğŸ“¦ Loading YOLOv8 model...")
        self.yolo = YOLO("yolov8n.pt")
        print("âœ… YOLO model loaded successfully!")

        # âœ… ì›¹ìº  ì—°ê²°
        self.cap = cv2.VideoCapture(0)
        if not self.cap.isOpened():
            print("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.")
            sys.exit(1)
        else:
            print("ğŸ¥ ì›¹ìº  ì—°ê²° ì„±ê³µ!")

        # âœ… ì‹¤í–‰ ìƒíƒœ
        self.running = True

        # âœ… ì…ë ¥ ìŠ¤ë ˆë“œ ì‹œì‘
        self.user_input_thread = threading.Thread(target=self._input_loop, daemon=True)
        self.user_input_thread.start()

    def _input_loop(self):
        while self.running:
            try:
                question = input("\nğŸ’¬ ì§ˆë¬¸ ì…ë ¥ (ì˜ˆ: Is there a chair? / ì¢…ë£Œ: exit): ").strip()
                if question.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
                    print("ğŸ›‘ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    self.clean_exit()
                    return

                ret, frame = self.cap.read()
                if not ret:
                    print("âš ï¸ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_image = PILImage.fromarray(rgb_image)

                # YOLO íƒì§€
                detections = self.run_yolo(frame)

                # BLIP ì§ˆë¬¸/ìº¡ì…˜
                if self.is_question_type(question):
                    answer = self.run_vqa(pil_image, question)
                else:
                    answer = self.run_captioning(pil_image)

                # ì‹œê°í™”
                self.visualize_result(frame, detections, answer)

            except KeyboardInterrupt:
                self.clean_exit()

    def run_yolo(self, frame):
        """YOLO íƒì§€"""
        results = self.yolo(frame, verbose=False)
        detections = []
        for box in results[0].boxes:
            x1, y1, x2, y2 = box.xyxy[0].tolist()
            conf = box.conf[0].item()
            cls = int(box.cls[0].item())
            label = self.yolo.names[cls]
            x_center = (x1 + x2) / 2
            y_center = (y1 + y2) / 2
            detections.append({
                'label': label,
                'conf': conf,
                'coords': (x_center, y_center)
            })
        return detections

    def run_vqa(self, pil_image, question):
        """BLIP VQA"""
        print(f"ğŸ§  [VQA] Q: {question}")
        inputs = self.vqa_processor(images=pil_image, text=question, return_tensors="pt").to(self.device)
        with torch.no_grad():
            output = self.vqa_model.generate(**inputs, max_new_tokens=50)
        answer = self.vqa_processor.decode(output[0], skip_special_tokens=True)
        print(f"âœ… A: {answer}")
        return answer

    def run_captioning(self, pil_image):
        """BLIP Captioning"""
        print("ğŸ–¼ï¸ [Captioning] Generating scene description...")
        inputs = self.caption_processor(images=pil_image, return_tensors="pt").to(self.device)
        with torch.no_grad():
            output = self.caption_model.generate(**inputs, max_new_tokens=50)
        caption = self.caption_processor.decode(output[0], skip_special_tokens=True)
        print(f"ğŸ“ Caption: {caption}")
        return caption

    def visualize_result(self, frame, detections, text):
        """YOLO + BLIP ê²°ê³¼ ì‹œê°í™”"""
        display_img = frame.copy()

        # ğŸ”¹ YOLO ë°”ìš´ë”©ë°•ìŠ¤
        for det in detections:
            label = det['label']
            conf = det['conf']
            (x_center, y_center) = det['coords']
            x1 = int(x_center - 40)
            y1 = int(y_center - 40)
            x2 = int(x_center + 40)
            y2 = int(y_center + 40)
            cv2.rectangle(display_img, (x1, y1), (x2, y2), (0, 255, 255), 2)
            cv2.putText(display_img, f"{label} ({conf:.2f})", (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)
            print(f"ğŸ“ {label}: ({x_center:.1f}, {y_center:.1f})")

        # ğŸ”¹ BLIP ë‹µë³€ í‘œì‹œ
        cv2.rectangle(display_img, (10, 10), (850, 80), (0, 0, 0), -1)
        cv2.putText(display_img, f"{text[:100]}", (20, 50),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.imshow('BLIP + YOLO Webcam Output', display_img)

        # ğŸ”¹ q í‚¤ë¡œ ì¢…ë£Œ
        if cv2.waitKey(1) & 0xFF == ord('q'):
            self.clean_exit()

    def is_question_type(self, text):
        """ë¬¼ìŒí˜• íŒë‹¨"""
        keywords = ["?", "ìˆ", "where", "what", "is there", "are there", "who"]
        return any(k in text.lower() for k in keywords)

    def clean_exit(self):
        """ì•ˆì „ ì¢…ë£Œ"""
        print("ğŸ§¹ ì¢…ë£Œ ì¤‘...")
        self.running = False
        if self.cap.isOpened():
            self.cap.release()
        cv2.destroyAllWindows()
        sys.exit(0)


if __name__ == "__main__":
    node = BLIP_YOLO_Webcam()
